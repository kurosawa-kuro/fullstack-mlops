name: ML Model CI/CD (DuckDB対応)

on:
  push:
    paths:
      - 'src/**'
      - 'src/configs/**'
      - 'src/ml/data/**'
      - 'src/ml/dwh/**'
      - 'src/tests/**'
      - 'requirements.txt'
      - '.github/workflows/ml-cicd.yml'
  pull_request:
    paths:
      - 'src/**'
      - 'src/configs/**'
      - 'src/ml/data/**'
      - 'src/ml/dwh/**'
      - 'src/tests/**'
      - 'requirements.txt'
      - '.github/workflows/ml-cicd.yml'
  workflow_dispatch:  # 手動実行可能

jobs:
  # モデル訓練（DuckDB DWH使用）- 全ブランチで実行
  train-model:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create necessary directories
        run: |
          mkdir -p src/ml/data/raw
          mkdir -p src/ml/data/dwh/scripts
          mkdir -p src/ml/data/dwh/data
          mkdir -p src/ml/data/dwh/core
          mkdir -p src/ml/data/dwh/config
          mkdir -p src/ml/models/trained

      - name: Create sample data
        run: |
          # サンプルデータを作成
          python -c "
          import pandas as pd
          import os
          
          # サンプルデータを作成
          sample_data = pd.DataFrame({
              'sqft': [1500, 2000, 1200, 1800, 2200, 1600, 2400, 1400, 1900, 2100],
              'bedrooms': [3, 4, 2, 3, 4, 3, 5, 2, 3, 4],
              'bathrooms': [2, 3, 1, 2, 3, 2, 4, 1, 2, 3],
              'year_built': [2010, 2015, 2008, 2012, 2018, 2011, 2020, 2009, 2013, 2017],
              'location': ['Suburban', 'Urban', 'Rural', 'Suburban', 'Urban', 'Suburban', 'Urban', 'Rural', 'Suburban', 'Urban'],
              'condition': ['Good', 'Excellent', 'Fair', 'Good', 'Excellent', 'Good', 'Excellent', 'Fair', 'Good', 'Excellent'],
              'price': [300000, 450000, 200000, 350000, 500000, 320000, 550000, 180000, 380000, 480000]
          })
          
          # 生データを保存
          os.makedirs('src/ml/data/raw', exist_ok=True)
          sample_data.to_csv('src/ml/data/raw/house_data.csv', index=False)
          print('✅ サンプルデータを作成しました')
          "

      - name: Build DuckDB DWH
        run: |
          # DWHスキーマを初期化
          python -c "
          import sys
          sys.path.insert(0, 'src')
          from ml.data.dwh.core.schema import create_schema
          from ml.data.dwh.core.database import DWHManager
          
          # DWHマネージャーを初期化
          dwh_manager = DWHManager('src/ml/data/dwh/data/house_price_dwh.duckdb')
          
          # スキーマを作成（テーブルとビューを含む）
          create_schema(dwh_manager)
          print('✅ DWHスキーマを初期化しました')
          "
          
          # DuckDB DWHを構築
          python src/ml/data/dwh/scripts/setup_dwh.py \
            --csv-file src/ml/data/raw/house_data.csv \
            --db-path src/ml/data/dwh/data/house_price_dwh.duckdb
          
          # Bronze層のデータをv_house_analyticsビューで使用できるように変換
          python -c "
          import sys
          sys.path.insert(0, 'src')
          import duckdb
          
          # DuckDBに接続
          conn = duckdb.connect('src/ml/data/dwh/data/house_price_dwh.duckdb')
          
          # まず、Bronze層のデータからディメンションテーブルにデータを挿入
          print('📊 ディメンションテーブルにデータを挿入中...')
          
          # ユニークなlocationを取得してdim_locationsに挿入
          locations = conn.execute('SELECT DISTINCT location FROM bronze_raw_house_data').fetchdf()
          for idx, row in locations.iterrows():
              location_name = row['location']
              location_type = 'Suburban' if 'suburban' in location_name.lower() else 'Urban' if 'urban' in location_name.lower() else 'Rural'
              conn.execute('''
                  INSERT OR IGNORE INTO dim_locations (location_id, location_name, location_type)
                  VALUES (?, ?, ?)
              ''', [idx + 1, location_name, location_type])
          
          # ユニークなconditionを取得してdim_conditionsに挿入
          conditions = conn.execute('SELECT DISTINCT condition FROM bronze_raw_house_data').fetchdf()
          condition_scores = {'Excellent': 5, 'Good': 4, 'Fair': 3, 'Poor': 2, 'Very Poor': 1}
          for idx, row in conditions.iterrows():
              condition_name = row['condition']
              condition_score = condition_scores.get(condition_name, 3)
              conn.execute('''
                  INSERT OR IGNORE INTO dim_conditions (condition_id, condition_name, condition_score)
                  VALUES (?, ?, ?)
              ''', [idx + 1, condition_name, condition_score])
          
          # ユニークなyear_builtを取得してdim_yearsに挿入
          years = conn.execute('SELECT DISTINCT year_built FROM bronze_raw_house_data').fetchdf()
          for idx, row in years.iterrows():
              year_value = row['year_built']
              decade = f\"{year_value // 10 * 10}s\"
              century = f\"{year_value // 100 + 1}th Century\"
              conn.execute('''
                  INSERT OR IGNORE INTO dim_years (year_id, year_value, decade, century)
                  VALUES (?, ?, ?, ?)
              ''', [idx + 1, year_value, decade, century])
          
          print('✅ ディメンションテーブルにデータを挿入しました')
          
          # 次に、Bronze層のデータをfact_house_transactionsに挿入
          print('📊 fact_house_transactionsにデータを挿入中...')
          conn.execute('''
            INSERT INTO fact_house_transactions 
            (transaction_id, house_id, location_id, condition_id, year_built_id, price, sqft, bedrooms, bathrooms)
            SELECT 
              b.id as transaction_id,
              b.id as house_id,
              l.location_id,
              c.condition_id,
              y.year_id,
              b.price, b.sqft, b.bedrooms, b.bathrooms
            FROM bronze_raw_house_data b
            JOIN dim_locations l ON b.location = l.location_name
            JOIN dim_conditions c ON b.condition = c.condition_name
            JOIN dim_years y ON b.year_built = y.year_value
          ''')
          
          print('✅ Bronze層のデータをfact_house_transactionsに変換しました')
          conn.close()
          "
          
          # DWHの状態確認
          python src/ml/data/dwh/scripts/setup_dwh.py \
            --db-path src/ml/data/dwh/data/house_price_dwh.duckdb \
            --validate-only

      - name: Run DuckDB-based ML pipeline
        run: |
          python src/ml/models/train_model.py \
            --config src/configs/model_config.yaml \
            --duckdb-path src/ml/data/dwh/data/house_price_dwh.duckdb \
            --models-dir src/ml/models \
            --view-name v_house_analytics

      - name: Verify model artifacts
        run: |
          ls -la src/ml/models/trained/
          python -c "
          import joblib
          model = joblib.load('src/ml/models/trained/house_price_prediction.pkl')
          preprocessor = joblib.load('src/ml/models/trained/house_price_prediction_encoders.pkl')
          print('✅ モデルと前処理器の読み込み成功')
          "

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: trained-models-${{ github.sha }}
          path: |
            src/ml/models/trained/house_price_prediction.pkl
            src/ml/models/trained/house_price_prediction_encoders.pkl
          retention-days: 30

      - name: Upload DuckDB database
        uses: actions/upload-artifact@v4
        with:
          name: duckdb-dwh-${{ github.sha }}
          path: src/ml/data/dwh/data/house_price_dwh.duckdb
          retention-days: 30

  # テスト実行（DuckDB DWH構築後のテスト）
  test:
    runs-on: ubuntu-latest
    needs: train-model
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          pip install -r requirements.txt

      - name: Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ github.sha }}
          path: src/ml/models/trained/

      - name: Download DuckDB database
        uses: actions/download-artifact@v4
        with:
          name: duckdb-dwh-${{ github.sha }}
          path: src/ml/data/dwh/data/

      - name: Run tests
        run: |
          pytest src/tests/test_ml_pipeline.py -v --cov=src --cov-report=xml --cov-report=html

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # モデル性能テスト（DuckDB対応）- 全ブランチで実行
  model-performance:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ github.sha }}
          path: src/ml/models/trained/

      - name: Download DuckDB database
        uses: actions/download-artifact@v4
        with:
          name: duckdb-dwh-${{ github.sha }}
          path: src/ml/data/dwh/data/

      - name: Run model performance tests
        run: |
          pytest src/tests/test_ml_pipeline.py::TestModelPipeline::test_model_files_exist -v
          pytest src/tests/test_ml_pipeline.py::TestModelPipeline::test_model_can_load -v
          pytest src/tests/test_ml_pipeline.py::TestModelPipeline::test_model_can_predict -v

      - name: Test DuckDB integration
        run: |
          python -c "
          import duckdb
          import joblib
          import pandas as pd
          
          # DuckDBからデータを読み込み
          conn = duckdb.connect('src/ml/data/dwh/data/house_price_dwh.duckdb')
          data = conn.execute('SELECT * FROM v_house_analytics LIMIT 5').fetchdf()
          conn.close()
          
          # モデルで予測
          model = joblib.load('src/ml/models/trained/house_price_prediction.pkl')
          preprocessor = joblib.load('src/ml/models/trained/house_price_prediction_encoders.pkl')
          
          # サンプルデータで予測
          sample = data.iloc[0:1]
          X = pd.DataFrame({
              'sqft': sample['sqft'],
              'bedrooms': sample['bedrooms'],
              'bathrooms': sample['bathrooms'],
              'house_age': sample['house_age'],
              'price_per_sqft': sample['price'] / sample['sqft'],
              'bed_bath_ratio': sample['bedrooms'] / sample['bathrooms'],
              'location': sample['location_name'],
              'condition': sample['condition_name']
          })
          
          X_transformed = preprocessor.transform(X)
          prediction = model.predict(X_transformed)
          
          print(f'✅ DuckDB統合テスト成功')
          print(f'📊 サンプル予測結果: ${prediction[0]:,.2f}')
          "

      - name: Model performance summary
        run: |
          echo "🎯 モデル性能テスト完了（DuckDB対応）"
          echo "📊 モデルファイルサイズ: $(ls -lh src/ml/models/trained/house_price_prediction.pkl | awk '{print $5}')"
          echo "📊 前処理器ファイルサイズ: $(ls -lh src/ml/models/trained/house_price_prediction_encoders.pkl | awk '{print $5}')"
          echo "📊 DuckDBデータベースサイズ: $(ls -lh src/ml/data/dwh/data/house_price_dwh.duckdb | awk '{print $5}')"

  # GitHub Release作成（タグプッシュ時のみ）
  create-release:
    runs-on: ubuntu-latest
    needs: [train-model, model-performance]
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download model artifacts
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ github.sha }}
          path: src/ml/models/trained/

      - name: Download DuckDB database
        uses: actions/download-artifact@v4
        with:
          name: duckdb-dwh-${{ github.sha }}
          path: src/ml/data/dwh/data/

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          files: |
            src/ml/models/trained/house_price_prediction.pkl
            src/ml/models/trained/house_price_prediction_encoders.pkl
            src/ml/data/dwh/data/house_price_dwh.duckdb
          body: |
            ## 🏠 House Price Prediction Model Release (DuckDB対応)
            
            ### 📦 含まれるファイル
            - `house_price_prediction.pkl`: 学習済みモデル
            - `house_price_prediction_encoders.pkl`: 前処理器
            - `house_price_dwh.duckdb`: DuckDBデータウェアハウス
            
            ### 🚀 使用方法
            ```python
            import joblib
            import duckdb
            
            # モデルと前処理器を読み込み
            model = joblib.load('house_price_prediction.pkl')
            preprocessor = joblib.load('house_price_prediction_encoders.pkl')
            
            # DuckDBからデータを読み込み
            conn = duckdb.connect('house_price_dwh.duckdb')
            data = conn.execute('SELECT * FROM v_house_analytics').fetchdf()
            conn.close()
            
            # 予測実行
            # (DuckDBデータで前処理 → 予測)
            ```
            
            ### 📊 モデル情報
            - アルゴリズム: Random Forest
            - 特徴量: sqft, bedrooms, bathrooms, house_age, price_per_sqft, bed_bath_ratio, location, condition
            - ターゲット: house price
            - データソース: DuckDB DWH (v_house_analytics view)
            
            ### 🔄 CI/CD
            このリリースは自動CI/CDパイプラインにより生成されました。
            DuckDBベースのデータウェアハウス構築からモデル訓練まで自動化されています。
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 